{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from statistics import mean,stdev\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b(i)-(iii)\n",
    "neg_files = glob.glob('C:\\\\Users\\\\win10\\\\Desktop\\\\Final\\\\neg\\\\cv*.txt')\n",
    "pos_files = glob.glob('C:\\\\Users\\\\win10\\\\Desktop\\\\Final\\\\pos\\\\cv*.txt')\n",
    "label_path = {1:[i for i in pos_files],0:[i for i in neg_files]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lst = [i for i in pos_files]\n",
    "pos_txt = []\n",
    "for i in pos_lst:\n",
    "    f = open(i, \"r\",encoding='utf-8')\n",
    "    content = f.read()\n",
    "    no_punc = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    no_num = re.sub(r'[\\d+]', '', no_punc).split()\n",
    "    pos_txt = pos_txt + no_num\n",
    "# print(pos_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_lst = [i for i in neg_files]\n",
    "neg_txt = []\n",
    "for i in neg_lst:\n",
    "    f = open(i, \"r\",encoding='utf-8')\n",
    "    content = f.read()\n",
    "    no_punc = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    no_num = re.sub(r'[\\d+]', '', no_punc).split()\n",
    "    neg_txt = neg_txt + no_num\n",
    "#print(neg_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test data\n",
    "train_files = neg_files[:700] + pos_files[:700] # len(trainn_data):1400\n",
    "test_files = neg_files[700:] + pos_files[700:] # len(test_data):600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word = []\n",
    "for i in train_files:\n",
    "    f = open(i,\"r\",encoding='utf-8')\n",
    "    content = f.read()\n",
    "    no_punc = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    no_num = re.sub(r'[\\d+]', '', no_punc)\n",
    "    train_word = train_word + no_num.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = []\n",
    "for i in test_files:\n",
    "    f = open(i,\"r\",encoding='utf-8')\n",
    "    content = f.read()\n",
    "    no_punc = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    no_num = re.sub(r'[\\d+]', '', no_punc)\n",
    "    test_word = test_word + no_num.split()\n",
    "#print(test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b(iv) Count the number of unique words in the whole dataset (train + test)\n",
    "\n",
    "word_lst = train_word + test_word\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words: 38911\n"
     ]
    }
   ],
   "source": [
    "unique_count = len(tokenizer.word_counts)\n",
    "print('The number of unique words:',unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average review length: 644.3575 \n",
      " standard deviation: 285.05139508985275\n"
     ]
    }
   ],
   "source": [
    "# b(v) Calculate the average review length and the standard deviation of review lengths.\n",
    "files = train_files + test_files\n",
    "review_len = []\n",
    "for i in files:\n",
    "    with open(i,\"r\",encoding='utf-8') as file:\n",
    "        content = file.read().split(\"\\n\")\n",
    "        \n",
    "    item = \"\"\n",
    "    for j in content:\n",
    "        item = item + j\n",
    "        \n",
    "    no_punc = re.sub(r'[^\\w\\s+]', '',item)\n",
    "    no_num = re.sub(r'[\\d+]', '', no_punc).split()\n",
    "    \n",
    "    review_len.append(len(no_num))\n",
    "\n",
    "print('average review length:', mean(review_len),'\\n','standard deviation:',stdev(review_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# b(vi) Plot the histogram of review lengths.\n",
    "fig, ax = plt.subplots(figsize =(10, 6))\n",
    "ax.hist(review_len, bins=100)\n",
    "# plt.hist(review_len, bins=100)\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('number of files')\n",
    "plt.title('Review length on whole dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b(vii) Tokenize each text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_txt tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = []\n",
    "for i in train_files:\n",
    "    with open(i,\"r\",encoding='utf-8') as file:\n",
    "        content = file.read().split(\"\\n\")\n",
    "        \n",
    "    item = \"\"\n",
    "    for j in content:\n",
    "        item = item + j\n",
    "        \n",
    "    no_punc = re.sub(r'[^\\w\\s+]', '',item)\n",
    "    no_num = re.sub(r'[\\d+]', '', no_punc)\n",
    "    \n",
    "    train_txt.append(str(no_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token = Tokenizer() # create the tokenizer\n",
    "train_token.fit_on_texts(train_txt)\n",
    "#print(train_token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_txt tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = []\n",
    "for i in test_files:\n",
    "    with open(i,\"r\",encoding='utf-8') as file:\n",
    "        content = file.read().split(\"\\n\")\n",
    "        \n",
    "    item = \"\"\n",
    "    for j in content:\n",
    "        item = item + j\n",
    "        \n",
    "    no_punc = re.sub(r'[^\\w\\s+]', '',item)\n",
    "    no_num = re.sub(r'[\\d+]', '', no_punc)\n",
    "    \n",
    "    test_txt.append(str(no_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = Tokenizer() # create the tokenizer\n",
    "test_token.fit_on_texts(test_txt)\n",
    "#print(test_token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b(viii)-(ix) \n",
    "# Select a review length L that 70% of the reviews have a length below it.\n",
    "# Truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_70 = sorted(review_len)[1400] # 737\n",
    "L_90 = sorted(review_len)[1800] # 997\n",
    "trun_lst = [L_70 if i> L_70 else i for i in review_len]\n",
    "\n",
    "# train set\n",
    "train_encoded = [one_hot(i,n = len(unique_lst)) for i in train_txt]\n",
    "# test set\n",
    "test_encoded = [one_hot(i,n = len(unique_lst)) for i in test_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-pad reviews\n",
    "train_padded = pad_sequences(train_encoded, maxlen = L_70)\n",
    "test_padded = pad_sequences(test_encoded, maxlen = L_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sentiment labelled as 0\n",
    "neg_labels = np.zeros(1000)\n",
    "train_neg_labels = neg_labels[:700]\n",
    "test_neg_labels = neg_labels[700:]\n",
    "\n",
    "# positive sentiment labelled as 1\n",
    "pos_labels = np.ones(1000)\n",
    "train_pos_labels = pos_labels[:700]\n",
    "test_pos_labels = pos_labels[700:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine negative and positive in train set: Y\n",
    "train_labels = np.concatenate((train_neg_labels, train_pos_labels))\n",
    "test_labels = np.concatenate((test_neg_labels, test_pos_labels))\n",
    "\n",
    "#y_train = pd.DataFrame(train_labels)\n",
    "#y_test = pd.DataFrame(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode train x and test x with top_word=5000: X\n",
    "train_encoded = [one_hot(i,n = 5000) for i in train_txt]\n",
    "test_encoded = [one_hot(i,n = 5000) for i in test_txt]\n",
    "\n",
    "train_padded = pad_sequences(train_encoded, maxlen = L_70)\n",
    "test_padded = pad_sequences(test_encoded, maxlen = L_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.9671\n",
      "Accuracy: 96.714288\n"
     ]
    }
   ],
   "source": [
    "# c (i) Define the model\n",
    "# model.add(Embedding(top words, 32, input length=max words)), where top_words=5,000 and max words=L.\n",
    "# in this case, L = L_70\n",
    "model = Sequential()\n",
    "model.add(Embedding(5001, 32, input_length= L_70))\n",
    "\n",
    "# c (ii) Flatten the matrix of each document to a vector\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "train_x = train_padded\n",
    "train_y = train_labels\n",
    "model.fit(train_x, train_y, epochs=2, verbose=0)\n",
    "loss, accuracy = model.evaluate(train_x, train_y)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_21 (Embedding)    (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 23584)             0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 1)                 23585     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,617\n",
      "Trainable params: 183,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReference: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\\n           https://github.com/codebasics/deep-learning-keras-tf-tutorial/blob/master/22_word_embedding/supervised_word_embeddings.ipynb\\n           https://www.youtube.com/watch?v=Wp-Wb456kSU\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reference: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "           https://github.com/codebasics/deep-learning-keras-tf-tutorial/blob/master/22_word_embedding/supervised_word_embeddings.ipynb\n",
    "           https://www.youtube.com/watch?v=Wp-Wb456kSU\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d (i) Train a MLP with 3 (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. \n",
    "#       Use a dropout rate of 20% for the first layer and 50% for the other layers. \n",
    "#       Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). \n",
    "#       To avoid overfitting, just set the number of epochs as 2. \n",
    "#       Use a batch size of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_22 (Embedding)    (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 23584)             0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 50)                1179250   \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,344,433\n",
      "Trainable params: 1,344,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5001, 32, input_length= L_70))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2)) # tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 8ms/step - loss: 0.6993 - accuracy: 0.5221 - val_loss: 0.6902 - val_accuracy: 0.5517\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.6181 - accuracy: 0.6429 - val_loss: 0.6455 - val_accuracy: 0.6467\n",
      "44/44 [==============================] - 0s 4ms/step - loss: 0.2403 - accuracy: 0.9314\n",
      "Accuracy: 93.142855\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "train_x = train_padded\n",
    "train_y = train_labels\n",
    "\n",
    "test_x = test_padded\n",
    "test_y = test_labels\n",
    "\n",
    "model.fit(train_x,train_y,epochs = 2, batch_size = 10, verbose = 1,validation_data = (test_x,test_y))\n",
    "loss, accuracy = model.evaluate(train_x, train_y)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step - loss: 0.6455 - accuracy: 0.6467\n",
      "Test Accuracy: 64.666665\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "loss,accuracy = model.evaluate(test_x,test_y)\n",
    "print('Test Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReference: https://github.com/KirillShmilovich/MLP-Neural-Network-From-Scratch/blob/master/MLP.ipynb\\n           https://github.com/rishabhrastogi31/student-admission-classifier-based-on-MLP-KERAS/blob/master/StudentAdmissionsKeras.ipynb\\n'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reference: https://github.com/KirillShmilovich/MLP-Neural-Network-From-Scratch/blob/master/MLP.ipynb\n",
    "           https://github.com/rishabhrastogi31/student-admission-classifier-based-on-MLP-KERAS/blob/master/StudentAdmissionsKeras.ipynb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Dimensional Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e (i) After the embedding layer, insert a Conv1D layer. \n",
    "#      This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, \n",
    "#      i.e. reads embedded word representations 3 vector elements of the word embedding at a time. \n",
    "#      The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 \n",
    "#      The rest of the network is the same as the neural network above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_23 (Embedding)    (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 735, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 367, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 11744)             0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 50)                587250    \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 755,537\n",
      "Trainable params: 755,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 9ms/step - loss: 0.6984 - accuracy: 0.5029\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.6516 - accuracy: 0.6093\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 0.3904 - accuracy: 0.8907\n",
      "Accuracy: 89.071429\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5001, 32, input_length= L_70))\n",
    "model.add(Conv1D(32,kernel_size = 3))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=2, padding='valid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_x,train_y,epochs = 2, batch_size = 10, verbose = 1)\n",
    "loss, accuracy = model.evaluate(train_x, train_y)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 6ms/step - loss: 0.6577 - accuracy: 0.5983\n",
      "Test Accuracy: 59.833336\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "loss,accuracy = model.evaluate(test_x,test_y)\n",
    "print('Test Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(train_x,train_y,test_x,test_y):\n",
    "    verbose, epochs, batch_size = 1, 2, 10\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(5001, 32, input_length= L_70))\n",
    "    model.add(Conv1D(32,kernel_size = 3))\n",
    "    model.add(MaxPooling1D(pool_size=2,strides=2, padding='valid'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_x,train_y,epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "    loss, accuracy = model.evaluate(train_x, train_y)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    test_loss,test_accuracy = model.evaluate(test_x,test_y)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 9ms/step - loss: 0.6962 - accuracy: 0.5121\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.6721 - accuracy: 0.5829\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 0.5519 - accuracy: 0.8379\n",
      "Accuracy: 83.785713\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.6703 - accuracy: 0.5917\n",
      "Test Accuracy: 59.166664\n"
     ]
    }
   ],
   "source": [
    "cnn_model(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReference: https://keras.io/api/layers/pooling_layers/max_pooling1d/\\n           https://keras.io/api/layers/convolution_layers/convolution1d/\\n           https://github.com/Gruschtel/1D-CNN/blob/master/1D_CNN_01.ipynb\\n           https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/\\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reference: https://keras.io/api/layers/pooling_layers/max_pooling1d/\n",
    "           https://keras.io/api/layers/convolution_layers/convolution1d/\n",
    "           https://github.com/Gruschtel/1D-CNN/blob/master/1D_CNN_01.ipynb\n",
    "           https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f (i) Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. \n",
    "#       Use a dropout rate of 0.2 for both LSTM and the dense layer. \n",
    "#       Train the model using 10-50 epochs and batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the epoch size = 15\n",
    "def lstm_model(train_x,train_y,test_x,test_y):\n",
    "    verbose, batch_size = 1, 10\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(5001, 32, input_length= L_70))\n",
    "    model.add(LSTM(32, dropout=0.2))\n",
    "    model.add(Dense(256,activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(train_x,train_y, epochs = 15, batch_size = batch_size, verbose = verbose,validation_data = (test_x,test_y))\n",
    "    \n",
    "    loss, accuracy = model.evaluate(train_x, train_y)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    test_loss,test_accuracy = model.evaluate(test_x,test_y)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_25 (Embedding)    (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177,057\n",
      "Trainable params: 177,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "140/140 [==============================] - 23s 156ms/step - loss: 0.6906 - accuracy: 0.5293 - val_loss: 0.6700 - val_accuracy: 0.6167\n",
      "Epoch 2/15\n",
      "140/140 [==============================] - 22s 154ms/step - loss: 0.5369 - accuracy: 0.7671 - val_loss: 0.6599 - val_accuracy: 0.6633\n",
      "Epoch 3/15\n",
      "140/140 [==============================] - 20s 142ms/step - loss: 0.2217 - accuracy: 0.9179 - val_loss: 0.6455 - val_accuracy: 0.6900\n",
      "Epoch 4/15\n",
      "140/140 [==============================] - 20s 140ms/step - loss: 0.0675 - accuracy: 0.9814 - val_loss: 0.9226 - val_accuracy: 0.7033\n",
      "Epoch 5/15\n",
      "140/140 [==============================] - 20s 143ms/step - loss: 0.0591 - accuracy: 0.9821 - val_loss: 1.0332 - val_accuracy: 0.6883\n",
      "Epoch 6/15\n",
      "140/140 [==============================] - 21s 147ms/step - loss: 0.0182 - accuracy: 0.9943 - val_loss: 1.1392 - val_accuracy: 0.6967\n",
      "Epoch 7/15\n",
      "140/140 [==============================] - 21s 149ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 1.3127 - val_accuracy: 0.7083\n",
      "Epoch 8/15\n",
      "140/140 [==============================] - 20s 142ms/step - loss: 5.8363e-04 - accuracy: 1.0000 - val_loss: 1.4437 - val_accuracy: 0.7150\n",
      "Epoch 9/15\n",
      "140/140 [==============================] - 21s 147ms/step - loss: 4.4518e-04 - accuracy: 1.0000 - val_loss: 1.5205 - val_accuracy: 0.7050\n",
      "Epoch 10/15\n",
      "140/140 [==============================] - 22s 155ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 1.4667 - val_accuracy: 0.6583\n",
      "Epoch 11/15\n",
      "140/140 [==============================] - 21s 153ms/step - loss: 0.0078 - accuracy: 0.9986 - val_loss: 1.5955 - val_accuracy: 0.6533\n",
      "Epoch 12/15\n",
      "140/140 [==============================] - 21s 147ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 1.6348 - val_accuracy: 0.6917\n",
      "Epoch 13/15\n",
      "140/140 [==============================] - 20s 141ms/step - loss: 0.0200 - accuracy: 0.9921 - val_loss: 1.7234 - val_accuracy: 0.6667\n",
      "Epoch 14/15\n",
      "140/140 [==============================] - 20s 145ms/step - loss: 0.0353 - accuracy: 0.9886 - val_loss: 1.3638 - val_accuracy: 0.6850\n",
      "Epoch 15/15\n",
      "140/140 [==============================] - 21s 150ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 1.4450 - val_accuracy: 0.6750\n",
      "44/44 [==============================] - 3s 54ms/step - loss: 0.0081 - accuracy: 0.9971\n",
      "Accuracy: 99.714285\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 1.4450 - accuracy: 0.6750\n",
      "Test Accuracy: 67.500001\n"
     ]
    }
   ],
   "source": [
    "lstm_model(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using 10-50 epochs and batch size of 10.\n",
    "def lstm_model_lst(train_x,train_y,test_x,test_y):\n",
    "    epochs_lst = [10,20,30,40,50]\n",
    "    verbose, batch_size = 0, 10\n",
    "    \n",
    "    for epoch in epochs_lst:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(5001, 32, input_length= L_70))\n",
    "        model.add(LSTM(32,dropout=0.2))\n",
    "        model.add(Dense(256,activation = 'relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(train_x,train_y, epochs = epoch, batch_size = batch_size, verbose = verbose)\n",
    "    \n",
    "        loss, accuracy = model.evaluate(train_x, train_y)\n",
    "        print('Accuracy: %f' % (accuracy*100))\n",
    "        test_loss,test_accuracy = model.evaluate(test_x,test_y)\n",
    "        print('Test Accuracy: %f' % (test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 2s 46ms/step - loss: 4.5660e-04 - accuracy: 1.0000\n",
      "Accuracy: 100.000000\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 1.7617 - accuracy: 0.7133\n",
      "Test Accuracy: 71.333331\n",
      "44/44 [==============================] - 2s 46ms/step - loss: 5.9253e-06 - accuracy: 1.0000\n",
      "Accuracy: 100.000000\n",
      "19/19 [==============================] - 1s 57ms/step - loss: 2.1362 - accuracy: 0.7000\n",
      "Test Accuracy: 69.999999\n",
      "44/44 [==============================] - 3s 55ms/step - loss: 3.2894e-06 - accuracy: 1.0000\n",
      "Accuracy: 100.000000\n",
      "19/19 [==============================] - 1s 44ms/step - loss: 2.6725 - accuracy: 0.6767\n",
      "Test Accuracy: 67.666668\n",
      "44/44 [==============================] - 2s 47ms/step - loss: 5.1804e-07 - accuracy: 1.0000\n",
      "Accuracy: 100.000000\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 2.2918 - accuracy: 0.7317\n",
      "Test Accuracy: 73.166668\n",
      "44/44 [==============================] - 3s 48ms/step - loss: 2.3403e-07 - accuracy: 1.0000\n",
      "Accuracy: 100.000000\n",
      "19/19 [==============================] - 1s 59ms/step - loss: 2.6440 - accuracy: 0.6983\n",
      "Test Accuracy: 69.833332\n"
     ]
    }
   ],
   "source": [
    "lstm_model_lst(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reference: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
